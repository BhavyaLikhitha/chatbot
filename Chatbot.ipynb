{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhavyaLikhitha/chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzy62sMhQgHT"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "upload=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG9-1iz_xHaQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCfs3rfxQdkF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VXpApJuPIO8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3spm9oSHxHaT"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,vocab_size,embedding, encoder_units,batch_size):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = encoder_units\n",
        "        self.embedding=  tf.keras.layers.Embedding(vocab_size,embedding)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,return_sequences=True,return_state=True,kernel_regularizer=tf.keras.regularizers.L2(0.0001))\n",
        "    def call(self,inputs, hidden_state):\n",
        "        embedded_input = self.embedding(inputs)\n",
        "        enc_outputs, thought_vector = self.gru(embedded_input, initial_state = hidden_state)\n",
        "        return enc_outputs, thought_vector"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "g5-dFJWhxHaU"
      },
      "source": [
        "outputs: 10,20,30,40,50\n",
        "priority: 0.3,0.1,0.1,0.3,0.2\n",
        "\n",
        "attention output: 3+2+3+12+10=30\n",
        "\n",
        "\n",
        "\n",
        "embedding - [0.1 -0.2 0.3 -0.02 -0.03]\n",
        "[0.01 0.002 0.003 0.04 0.005 0.9 0.003 0.002 0.001 0.003]\n",
        "[he she it they them are this that you their]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4O54VNGqMnx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjjwLjgHxHaW"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self,units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_output_layer = tf.keras.layers.Dense(units,kernel_regularizer=tf.keras.regularizers.L2(0.0001))\n",
        "        self.thought_layer    = tf.keras.layers.Dense(units,kernel_regularizer=tf.keras.regularizers.L2(0.0001))\n",
        "        self.final_layer      = tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.L2(0.0001))\n",
        "    def call(self, enc_outputs, thought_vector): \n",
        "        thought_matrix = tf.expand_dims(thought_vector,1)\n",
        "        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n",
        "        attention_weights = tf.keras.activations.softmax(scores,axis=-1)\n",
        "        attention_output = attention_weights * enc_outputs #shape(batch_size,num_outputs,output_size)\n",
        "        attention_output = tf.reduce_sum(attention_output, axis=1)#new shape (batch size, output_size)\n",
        "        return attention_output, attention_weights\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nRzxgYixHaY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,vocab_size,embedding,decoder_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units  = decoder_units\n",
        "        self.embedding  = tf.keras.layers.Embedding(vocab_size, embedding)\n",
        "        self.gru        = tf.keras.layers.GRU(self.dec_units,return_sequences=True,return_state=True,kernel_regularizer=tf.keras.regularizers.L2(0.0001))\n",
        "        self.attention  = Attention(self.dec_units)\n",
        "        self.word_output= tf.keras.layers.Dense(vocab_size,kernel_regularizer=tf.keras.regularizers.L2(0.0001))\n",
        "    def call(self,inputs, enc_outputs, thought_vector):\n",
        "        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n",
        "        embedded_inputs = self.embedding(inputs)# shape(batch_size,num_words, size_of_embedding)\n",
        "        attention_output = tf.expand_dims(attention_output,1)#new shape (batch size,1, output_size)\n",
        "        concat_inputs  = tf.concat([attention_output,embedded_inputs], axis=-1)\n",
        "        \n",
        "        decoder_outputs, hidden_state = self.gru(concat_inputs)#shape (batch size,1, output_size)\n",
        "        decoder_outputs = tf.reshape(decoder_outputs,(-1,decoder_outputs.shape[2]))#shape (batch size, output_size)\n",
        "        final_outputs = self.word_output(decoder_outputs)\n",
        "        return final_outputs, hidden_state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNw6A1pixHaY"
      },
      "outputs": [],
      "source": [
        " class Train:\n",
        "    def __init__(self):\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    def loss_function(self, y_real,y_pred):\n",
        "        base_mask = tf.math.logical_not(tf.math.equal(y_real,0))\n",
        "        base_loss = self.base_loss_function(y_real,y_pred)\n",
        "        \n",
        "        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n",
        "        final_loss = mask * base_loss\n",
        "        return tf.reduce_mean(final_loss)\n",
        "    def train_step(self, train_data,label_data,enc_hidden, encoder, decoder, batch_size,label_tokenizer):\n",
        "        loss=0\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_outputs, thought_vector = encoder(train_data,enc_hidden)\n",
        "            dec_hidden = thought_vector\n",
        "            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "            \n",
        "            for index in range(1,label_data.shape[1]):\n",
        "                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs,dec_hidden)\n",
        "                dec_input = tf.expand_dims(label_data[:,index],1)\n",
        "                loss = loss + self.loss_function(label_data[:,index],outputs)\n",
        "            word_loss = loss/ int(label_data.shape[1])\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables  \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients,variables))\n",
        "        return word_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mxF-NvSxHaa"
      },
      "outputs": [],
      "source": [
        "class Data_Preprocessing:\n",
        "    def __init__(self):\n",
        "        self.temp = None\n",
        "    def get_data(self,path):\n",
        "        file = open(path, 'r').read()\n",
        "        lists = [f.split('\\t') for f in file.split('\\n')]\n",
        "        \n",
        "        questions = [x[0] for x in lists]\n",
        "        answers = [x[1] for x in lists]\n",
        "        return questions, answers\n",
        "    \n",
        "    def process_sentence(self,line):\n",
        "        line = line.lower().strip()\n",
        "        line = re.sub(r\"([?.,!])\", r\" \\1 \", line)\n",
        "        line = re.sub(r'[\" \"]+', \" \", line)\n",
        "        line = re.sub(r\"[^a-z?!,.']+\", \" \",line)\n",
        "        line = line.strip()\n",
        "        line = '<start> ' + line + ' <end>'\n",
        "        return line\n",
        "    def word_to_vec(self,inputs):\n",
        "        tokenizer = Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(inputs)\n",
        "        \n",
        "        vectors = tokenizer.texts_to_sequences(inputs)\n",
        "        vectors = pad_sequences(vectors,padding='post')\n",
        "        return vectors, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wec1noZAQZS0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4cmnXb4xHab"
      },
      "outputs": [],
      "source": [
        "data = Data_Preprocessing()\n",
        "questions, answers = data.get_data('/content/chatbot.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6U9QxwvPh2V"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHs4DGhaPiUL"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I3kvM-FxHac"
      },
      "outputs": [],
      "source": [
        "questions = [data.process_sentence(sentence) for sentence in questions]\n",
        "answers = [data.process_sentence(sentence) for sentence in answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY542TyExHac"
      },
      "outputs": [],
      "source": [
        "train_vectors, train_tokenizer = data.word_to_vec(questions)\n",
        "label_vectors, label_tokenizer = data.word_to_vec(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR2Ra8i2xHad"
      },
      "outputs": [],
      "source": [
        "max_length_train = train_vectors.shape[1]\n",
        "max_length_label = label_vectors.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EBJuu_lxHad"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "buffer_size = train_vectors.shape[0]\n",
        "embedding_dim = 256\n",
        "steps_per_epoch = buffer_size//batch_size\n",
        "units = 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdlHDgyBxHad"
      },
      "outputs": [],
      "source": [
        "vocab_train = len(train_tokenizer.word_index)+1\n",
        "vocab_label = len(label_tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMh1GPRixHae"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((train_vectors,label_vectors))\n",
        "dataset = dataset.shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z651JnlwxHae"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_train, embedding_dim, units , batch_size)\n",
        "decoder = Decoder(vocab_label , embedding_dim, units , batch_size)\n",
        "trainer  = Train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM1IehaIxHaf",
        "outputId": "ff16bdf4-d1ba-4d20-cc21-52f3deb2f5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.8355621099472046\n",
            "Epoch: 2, Loss: 1.5612375736236572\n",
            "Epoch: 3, Loss: 1.4261175394058228\n",
            "Epoch: 4, Loss: 1.3131062984466553\n",
            "Epoch: 5, Loss: 1.1974684000015259\n",
            "Epoch: 6, Loss: 1.0731583833694458\n"
          ]
        }
      ],
      "source": [
        "Epoch = 20\n",
        "for epoch in range(1, Epoch + 1):\n",
        "    enc_hidden = tf.zeros((batch_size, units))\n",
        "    total_loss = 0\n",
        "    for (batch, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n",
        "        total_loss = total_loss + batch_loss\n",
        "    \n",
        "    print(f'Epoch: {epoch}, Loss: {total_loss/ steps_per_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwVAD-B_79La"
      },
      "source": [
        "## Second Part ends here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYRzOsJGxHaf"
      },
      "outputs": [],
      "source": [
        "class Chatbot:\n",
        "    def __init__(self, encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units):\n",
        "        self.train_tokenizer = train_tokenizer\n",
        "        self.label_tokenizer = label_tokenizer\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.units = units\n",
        "        self.data = Data_Preprocessing()\n",
        "        self.max_len = max_length_train\n",
        "        \n",
        "    def clean_answer(self, answer):\n",
        "        answer = answer[:-1]\n",
        "        answer = ' '.join(answer)\n",
        "        return answer\n",
        "        \n",
        "    def predict(self, sentence):\n",
        "        sentence = self.data.process_sentence(sentence)\n",
        "        sentence_mat = []\n",
        "        for word in sentence.split(\" \"):\n",
        "            try:\n",
        "                sentence_mat.append(self.train_tokenizer.word_index[word])\n",
        "            except:\n",
        "                return \"could not understand that, can you re-phrase it?\"\n",
        "        sentence_mat = pad_sequences([sentence_mat],maxlen = self.max_len  ,padding= 'post')\n",
        "        sentence_mat = tf.convert_to_tensor(sentence_mat)\n",
        "        \n",
        "        enc_hidden = [tf.zeros((1, self.units))]\n",
        "        encoder_outputs, thought_vector = self.encoder(sentence_mat, enc_hidden)\n",
        "        dec_hidden = thought_vector\n",
        "        dec_input =  tf.expand_dims([label_tokenizer.word_index['<start>']], 0)\n",
        "        \n",
        "        answer = []\n",
        "        for i in range(1, self.max_len):\n",
        "            pred, dec_hidden,_ = decoder(dec_input, encoder_outputs,dec_hidden)\n",
        "            word = self.label_tokenizer.index_word[np.argmax(pred[0])]\n",
        "            answer.append(word)\n",
        "            if word =='<end>':\n",
        "                return self.clean_answer(answer)\n",
        "            dec_input = tf.expand_dims([np.argmax(pred[0])],0)\n",
        "            \n",
        "        return self.clean_answer(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y-MIOuIxHag"
      },
      "outputs": [],
      "source": [
        "bot = Chatbot(encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1D1GnPtxHag"
      },
      "outputs": [],
      "source": [
        "bot.predict('hi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ4IiswBxHah"
      },
      "outputs": [],
      "source": [
        "bot.predict('how are you')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov2qrCaOxHai"
      },
      "outputs": [],
      "source": [
        "bot.predict('hi, how are you ?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7Dnl5-GxHai"
      },
      "outputs": [],
      "source": [
        "question=''\n",
        "while True:\n",
        "    question = str(input('You:'))\n",
        "    if question == 'quit' or question == 'Quit':\n",
        "        break\n",
        "    answer = bot.predict(question)\n",
        "    print(f'Jarwis: {answer}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypr1qOQdxHaj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}